{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-FPcGDygUta"
      },
      "source": [
        "## Loick CUER et Léa FELDMAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTQlDgQY780x"
      },
      "source": [
        "## Natural Language Processing : Information Retrieval Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lu9668b7802"
      },
      "source": [
        "### Installation de BM25 et clonage du dossier de projet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0aLbmBFRRj",
        "outputId": "db6542af-20d3-44ec-e564-8db04b14512e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "fatal: destination path 'project1-2023' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25\n",
        "!git clone https://github.com/cr-nlp/project1-2023.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTqVp4v57804"
      },
      "source": [
        "### Importations des librairies principales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jElt0zoB7805",
        "outputId": "5c408c2d-423f-4f00-8b66-a4072819d5a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import urllib.request as url_request\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import ndcg_score\n",
        "from rank_bm25 import BM25Okapi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R8PkWU67805"
      },
      "source": [
        "Fonction qui charge le corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjKr16sB7806"
      },
      "outputs": [],
      "source": [
        "def loadNFCorpus():\n",
        "\tdir = \"./project1-2023/\"\n",
        "\tfilename = dir +\"dev.docs\"\n",
        "\n",
        "\tdicDoc={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\t#print(tabLine)\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\t#print(value)\n",
        "\t\tdicDoc[key] = value\n",
        "\tfilename = dir + \"dev.all.queries\"\n",
        "\tdicReq={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\tdicReq[key] = value\n",
        "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
        "\tdicReqDoc=defaultdict(dict)\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.strip().split('\\t')\n",
        "\t\treq = tabLine[0]\n",
        "\t\tdoc = tabLine[2]\n",
        "\t\tscore = int(tabLine[3])\n",
        "\t\tdicReqDoc[req][doc]=score\n",
        "\n",
        "\treturn dicDoc, dicReq, dicReqDoc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvEvYorN7806"
      },
      "source": [
        "## BM25 basique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45CKUrYP7807"
      },
      "outputs": [],
      "source": [
        "def text2TokenList(text):\n",
        "\tstopword = stopwords.words('english')\n",
        "\t#print(\"LEN DE STOPWORD=\",len(stopword))\n",
        "\tword_tokens = word_tokenize(text.lower())\n",
        "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
        "\treturn word_tokens_without_stops\n",
        "\n",
        "def run_bm25_only(startDoc,endDoc):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\t#150\n",
        "\tndcgTop=10\n",
        "\t#print(\"ndcgTop=\",ndcgTop,\"nbDocsToKeep=\",nbDocsToKeep)\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t#\"\"\"\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\t#print(\"doc vocab=\",len(allVocabListDoc))\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicReq[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tfrom rank_bm25 import BM25Okapi\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"reqs...\")\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = text2TokenList(dicReq[k])\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"bm25 doc indexing...\")\n",
        "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfrom sklearn.metrics import ndcg_score\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\t#get position docId\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\t\t\t\t\t#print(\"TOKEEP=\",docId)\n",
        "\t\t\t\t\t#print(trueDocs)\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\treturn ndcgBM25Cumul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omJ08cCb7807",
        "outputId": "542ccc8d-0089-4347-f10f-d82356b57a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultat en prenant seulement 150 documents en compte : 0.7519377590245511\n"
          ]
        }
      ],
      "source": [
        "print(\"Résultat en prenant seulement 150 documents en compte : \"+str(run_bm25_only(0,150)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA8zt0eL7808"
      },
      "source": [
        "### Pre-Processing alternatifs et BM25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tnmyou67808"
      },
      "source": [
        "La fonction qui se charge du pre-processing dans le code de base est text2TokenList. Apportons-y quelques changements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n56VR4j47808"
      },
      "outputs": [],
      "source": [
        "def text2TokenList(text, use_bigrams=True, use_trigrams=True):\n",
        "    stopword = stopwords.words('english') #load the list of english stopwords\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text) # delete characters that are not letters\n",
        "    text = text.lower() #convert words into lowercase\n",
        "    word_tokens = word_tokenize(text) # tokenize our sentences into tokens (words)\n",
        "    word_tokens_filtered = [word for word in word_tokens if word not in stopword and len(word) > 2] #filter stopwords and short words\n",
        "    lemmatizer = WordNetLemmatizer() #lemmatize our tokens\n",
        "    word_tokens_lemmatized = [lemmatizer.lemmatize(token) for token in word_tokens_filtered]\n",
        "    if use_bigrams: #add bigrams\n",
        "        bigrams = list(nltk.bigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(bigram) for bigram in bigrams])\n",
        "    if use_trigrams: #add trigrams\n",
        "        trigrams = list(nltk.trigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(trigram) for trigram in trigrams])\n",
        "    return word_tokens_lemmatized\n",
        "\n",
        "def run_bm25_only(startDoc,endDoc):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\t#150\n",
        "\tndcgTop=10\n",
        "\t#print(\"ndcgTop=\",ndcgTop,\"nbDocsToKeep=\",nbDocsToKeep)\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t#\"\"\"\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k],True,True)\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\t#print(\"doc vocab=\",len(allVocabListDoc))\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicReq[k],True,True)\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tfrom rank_bm25 import BM25Okapi\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k],True,True)\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"reqs...\")\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = text2TokenList(dicReq[k],True,True)\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"bm25 doc indexing...\")\n",
        "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfrom sklearn.metrics import ndcg_score\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\t#get position docId\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\t\t\t\t\t#print(\"TOKEEP=\",docId)\n",
        "\t\t\t\t\t#print(trueDocs)\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\treturn ndcgBM25Cumul\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhhAifVl7808",
        "outputId": "6fe5099a-3e37-404d-a05b-198a5eff0790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultat en prenant seulement 150 documents en compte : 0.7612331097196332\n"
          ]
        }
      ],
      "source": [
        "print(\"Résultat en prenant seulement 150 documents en compte : \"+str(run_bm25_only(0,150)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBBJx__J7809",
        "outputId": "e4be5a69-afd1-4c27-ccf1-3b55f79d2ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clé du premier document : MED-118\n",
            "Contenu du premier document : alkylphenols human milk relations dietary habits central taiwan pubmed ncbi abstract aims study determine concentrations num nonylphenol np num octylphenol op num human milk samples examine related factors including mothers demographics dietary habits women consumed median amount cooking oil significantly higher op concentrations num ng/g consumed num ng/g num op concentration significantly consumption cooking oil beta num num fish oil capsules beta num num adjustment age body mass index bmi np concentration significantly consumption fish oil capsules beta num num processed fish products beta num num food pattern cooking oil processed meat products factor analysis strongly op concentration human milk num determinations aid suggesting foods consumption nursing mothers order protect infants np/op exposure num elsevier rights reserved \n",
            "\n",
            "['alkylphenols', 'human', 'milk', 'relation', 'dietary', 'habit', 'central', 'taiwan', 'pubmed', 'ncbi', 'abstract', 'aim', 'study', 'determine', 'concentration', 'num', 'nonylphenol', 'num', 'octylphenol', 'num', 'human', 'milk', 'sample', 'examine', 'related', 'factor', 'including', 'mother', 'demographic', 'dietary', 'habit', 'woman', 'consumed', 'median', 'amount', 'cooking', 'oil', 'significantly', 'higher', 'concentration', 'num', 'consumed', 'num', 'num', 'concentration', 'significantly', 'consumption', 'cooking', 'oil', 'beta', 'num', 'num', 'fish', 'oil', 'capsule', 'beta', 'num', 'num', 'adjustment', 'age', 'body', 'mass', 'index', 'bmi', 'concentration', 'significantly', 'consumption', 'fish', 'oil', 'capsule', 'beta', 'num', 'num', 'processed', 'fish', 'product', 'beta', 'num', 'num', 'food', 'pattern', 'cooking', 'oil', 'processed', 'meat', 'product', 'factor', 'analysis', 'strongly', 'concentration', 'human', 'milk', 'num', 'determination', 'aid', 'suggesting', 'food', 'consumption', 'nursing', 'mother', 'order', 'protect', 'infant', 'exposure', 'num', 'elsevier', 'right', 'reserved', 'alkylphenols human', 'human milk', 'milk relations', 'relations dietary', 'dietary habits', 'habits central', 'central taiwan', 'taiwan pubmed', 'pubmed ncbi', 'ncbi abstract', 'abstract aims', 'aims study', 'study determine', 'determine concentrations', 'concentrations num', 'num nonylphenol', 'nonylphenol num', 'num octylphenol', 'octylphenol num', 'num human', 'human milk', 'milk samples', 'samples examine', 'examine related', 'related factors', 'factors including', 'including mothers', 'mothers demographics', 'demographics dietary', 'dietary habits', 'habits women', 'women consumed', 'consumed median', 'median amount', 'amount cooking', 'cooking oil', 'oil significantly', 'significantly higher', 'higher concentrations', 'concentrations num', 'num consumed', 'consumed num', 'num num', 'num concentration', 'concentration significantly', 'significantly consumption', 'consumption cooking', 'cooking oil', 'oil beta', 'beta num', 'num num', 'num fish', 'fish oil', 'oil capsules', 'capsules beta', 'beta num', 'num num', 'num adjustment', 'adjustment age', 'age body', 'body mass', 'mass index', 'index bmi', 'bmi concentration', 'concentration significantly', 'significantly consumption', 'consumption fish', 'fish oil', 'oil capsules', 'capsules beta', 'beta num', 'num num', 'num processed', 'processed fish', 'fish products', 'products beta', 'beta num', 'num num', 'num food', 'food pattern', 'pattern cooking', 'cooking oil', 'oil processed', 'processed meat', 'meat products', 'products factor', 'factor analysis', 'analysis strongly', 'strongly concentration', 'concentration human', 'human milk', 'milk num', 'num determinations', 'determinations aid', 'aid suggesting', 'suggesting foods', 'foods consumption', 'consumption nursing', 'nursing mothers', 'mothers order', 'order protect', 'protect infants', 'infants exposure', 'exposure num', 'num elsevier', 'elsevier rights', 'rights reserved', 'alkylphenols human milk', 'human milk relations', 'milk relations dietary', 'relations dietary habits', 'dietary habits central', 'habits central taiwan', 'central taiwan pubmed', 'taiwan pubmed ncbi', 'pubmed ncbi abstract', 'ncbi abstract aims', 'abstract aims study', 'aims study determine', 'study determine concentrations', 'determine concentrations num', 'concentrations num nonylphenol', 'num nonylphenol num', 'nonylphenol num octylphenol', 'num octylphenol num', 'octylphenol num human', 'num human milk', 'human milk samples', 'milk samples examine', 'samples examine related', 'examine related factors', 'related factors including', 'factors including mothers', 'including mothers demographics', 'mothers demographics dietary', 'demographics dietary habits', 'dietary habits women', 'habits women consumed', 'women consumed median', 'consumed median amount', 'median amount cooking', 'amount cooking oil', 'cooking oil significantly', 'oil significantly higher', 'significantly higher concentrations', 'higher concentrations num', 'concentrations num consumed', 'num consumed num', 'consumed num num', 'num num concentration', 'num concentration significantly', 'concentration significantly consumption', 'significantly consumption cooking', 'consumption cooking oil', 'cooking oil beta', 'oil beta num', 'beta num num', 'num num fish', 'num fish oil', 'fish oil capsules', 'oil capsules beta', 'capsules beta num', 'beta num num', 'num num adjustment', 'num adjustment age', 'adjustment age body', 'age body mass', 'body mass index', 'mass index bmi', 'index bmi concentration', 'bmi concentration significantly', 'concentration significantly consumption', 'significantly consumption fish', 'consumption fish oil', 'fish oil capsules', 'oil capsules beta', 'capsules beta num', 'beta num num', 'num num processed', 'num processed fish', 'processed fish products', 'fish products beta', 'products beta num', 'beta num num', 'num num food', 'num food pattern', 'food pattern cooking', 'pattern cooking oil', 'cooking oil processed', 'oil processed meat', 'processed meat products', 'meat products factor', 'products factor analysis', 'factor analysis strongly', 'analysis strongly concentration', 'strongly concentration human', 'concentration human milk', 'human milk num', 'milk num determinations', 'num determinations aid', 'determinations aid suggesting', 'aid suggesting foods', 'suggesting foods consumption', 'foods consumption nursing', 'consumption nursing mothers', 'nursing mothers order', 'mothers order protect', 'order protect infants', 'protect infants exposure', 'infants exposure num', 'exposure num elsevier', 'num elsevier rights', 'elsevier rights reserved']\n"
          ]
        }
      ],
      "source": [
        "dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "first_doc_key = next(iter(dicDoc))  #Let's take a look at the first document of our corpus and the output of our pre-processing applicated on this document\n",
        "\n",
        "print(f\"Clé du premier document : {first_doc_key}\")\n",
        "print(f\"Contenu du premier document : {first_doc_value}\")\n",
        "print(text2TokenList(first_doc_value, use_bigrams=True, use_trigrams=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2NBEQAL7809"
      },
      "source": [
        "Nous avons choisi la lemmatisation car elle est en général plus précise même si plus lente et plus complexe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPB8ZZ_N7809"
      },
      "source": [
        "### Creation du modèle word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xyy_8ppS7809",
        "outputId": "020f34cc-e874-4b5b-8b9b-224037e9211c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ79u5pn7809"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-msjKs58780-"
      },
      "outputs": [],
      "source": [
        "dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "def text2TokenList(text, use_bigrams=True, use_trigrams=True):\n",
        "    stopword = stopwords.words('english') #load the list of english stopwords\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text) # delete characters that are not letters\n",
        "    text = text.lower() #convert words into lowercase\n",
        "    word_tokens = word_tokenize(text) # tokenize our sentences into tokens (words)\n",
        "    word_tokens_filtered = [word for word in word_tokens if word not in stopword and len(word) > 2] #filter stopwords and short words\n",
        "    lemmatizer = WordNetLemmatizer() #lemmatize our tokens\n",
        "    word_tokens_lemmatized = [lemmatizer.lemmatize(token) for token in word_tokens_filtered]\n",
        "    if use_bigrams: #add bigrams\n",
        "        bigrams = list(nltk.bigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(bigram) for bigram in bigrams])\n",
        "    if use_trigrams: #add trigrams\n",
        "        trigrams = list(nltk.trigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(trigram) for trigram in trigrams])\n",
        "    return word_tokens_lemmatized\n",
        "\n",
        "# tokenize our corpus sentences to words\n",
        "tokenized_corpus = [text2TokenList(doc, use_bigrams=True, use_trigrams=True) for doc in list(dicDoc.values())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td4kMZKP780-"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(tokenized_corpus, vector_size=400, window=8, min_count=3, workers=6, sg=0, epochs=100) #create our word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYcGK-Jg780-"
      },
      "outputs": [],
      "source": [
        "bm25 = BM25Okapi(tokenized_corpus) #apply BM25 to our tokenized corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arG25zIW780-",
        "outputId": "73a4acc1-3aab-44a2-d42a-e9f613df00a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultat du modèle BM25 avec pre-processing et word2vec : 0.9477037341451985\n"
          ]
        }
      ],
      "source": [
        "def retrieve_documents_bm25(query, bm25, top_n=5):  #Tokenize the query, use BM25 model to compute relevance scores and return the top n documents with the best scores.\n",
        "    query_tokens = text2TokenList(query)\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "    return np.argsort(-bm25_scores)[:top_n]\n",
        "\n",
        "def evaluate_retrieval_bm25(dicReq, dicReqDoc, bm25, doc_ids, top_n=5): #Assesses the retrieval performance of a BM25 model by computing the mean Normalized Discounted Cumulative Gain (NDCG)\n",
        "    ndcg_scores = []\n",
        "    for req_id, query in dicReq.items():\n",
        "        true_scores_dict = dicReqDoc[req_id]\n",
        "        retrieved_doc_indices = retrieve_documents_bm25(query, bm25, top_n=top_n)\n",
        "        retrieved_doc_ids = [doc_ids[idx] for idx in retrieved_doc_indices]\n",
        "        retrieved_scores = np.array([true_scores_dict.get(doc_id, 0) for doc_id in retrieved_doc_ids], dtype=np.float64)\n",
        "        ideal_scores_list = sorted(true_scores_dict.values(), reverse=True)[:len(retrieved_scores)]\n",
        "        ideal_scores_list.extend([0] * (top_n - len(ideal_scores_list)))\n",
        "        ideal_scores = np.array(ideal_scores_list, dtype=np.float64)\n",
        "        ndcg = ndcg_score([ideal_scores], [retrieved_scores], k=top_n)\n",
        "        ndcg_scores.append(ndcg)\n",
        "    return np.mean(ndcg_scores)\n",
        "\n",
        "doc_ids = list(dicDoc.keys())\n",
        "bm25_ndcg = evaluate_retrieval_bm25(dicReq, dicReqDoc, bm25, doc_ids)\n",
        "print(\"Résultat du modèle BM25 avec pre-processing et word2vec : \"+str(bm25_ndcg))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
