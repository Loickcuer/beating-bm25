{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-FPcGDygUta"
      },
      "source": [
        "## Loick CUER et LÃ©a FELDMAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTQlDgQY780x"
      },
      "source": [
        "## Natural Language Processing : Information Retrieval Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lu9668b7802"
      },
      "source": [
        "### Installation de BM25 et clonage du dossier de projet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0aLbmBFRRj",
        "outputId": "db6542af-20d3-44ec-e564-8db04b14512e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank_bm25 in c:\\users\\loick\\desktop\\beating_bm25\\venv\\lib\\site-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\loick\\desktop\\beating_bm25\\venv\\lib\\site-packages (from rank_bm25) (2.1.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "Cloning into 'project1-2023'...\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25\n",
        "!git clone https://github.com/cr-nlp/project1-2023.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTqVp4v57804"
      },
      "source": [
        "### Importations des librairies principales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jElt0zoB7805",
        "outputId": "5c408c2d-423f-4f00-8b66-a4072819d5a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urllib.request as url_request\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import ndcg_score\n",
        "from rank_bm25 import BM25Okapi\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KjKr16sB7806"
      },
      "outputs": [],
      "source": [
        "def loadNFCorpus():\n",
        "\tdir = \"../data/raw/\"\n",
        "\tfilename = dir +\"dev.docs\"\n",
        "\n",
        "\tdicDoc={}\n",
        "\twith open(filename, 'r', encoding='utf-8') as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\t#print(tabLine)\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\t#print(value)\n",
        "\t\tdicDoc[key] = value\n",
        "\tfilename = dir + \"dev.all.queries\"\n",
        "\tdicReq={}\n",
        "\twith open(filename, 'r', encoding='utf-8') as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\tdicReq[key] = value\n",
        "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
        "\tdicReqDoc=defaultdict(dict)\n",
        "\twith open(filename, 'r', encoding='utf-8') as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.strip().split('\\t')\n",
        "\t\treq = tabLine[0]\n",
        "\t\tdoc = tabLine[2]\n",
        "\t\tscore = int(tabLine[3])\n",
        "\t\tdicReqDoc[req][doc]=score\n",
        "\n",
        "\treturn dicDoc, dicReq, dicReqDoc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvEvYorN7806"
      },
      "source": [
        "## BM25 basique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "45CKUrYP7807"
      },
      "outputs": [],
      "source": [
        "def text2TokenList(text):\n",
        "\tstopword = stopwords.words('english')\n",
        "\t#print(\"LEN DE STOPWORD=\",len(stopword))\n",
        "\tword_tokens = word_tokenize(text.lower())\n",
        "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
        "\treturn word_tokens_without_stops\n",
        "\n",
        "def run_bm25_only(startDoc,endDoc):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\t#150\n",
        "\tndcgTop=10\n",
        "\t#print(\"ndcgTop=\",ndcgTop,\"nbDocsToKeep=\",nbDocsToKeep)\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t#\"\"\"\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\t#print(\"doc vocab=\",len(allVocabListDoc))\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicReq[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tfrom rank_bm25 import BM25Okapi\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"reqs...\")\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = text2TokenList(dicReq[k])\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"bm25 doc indexing...\")\n",
        "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfrom sklearn.metrics import ndcg_score\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\t#get position docId\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\t\t\t\t\t#print(\"TOKEEP=\",docId)\n",
        "\t\t\t\t\t#print(trueDocs)\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\treturn ndcgBM25Cumul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omJ08cCb7807",
        "outputId": "542ccc8d-0089-4347-f10f-d82356b57a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RÃ©sultat en prenant seulement 150 documents en compte : 0.751937759024551\n"
          ]
        }
      ],
      "source": [
        "print(\"RÃ©sultat en prenant seulement 150 documents en compte : \"+str(run_bm25_only(0,150)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA8zt0eL7808"
      },
      "source": [
        "### Pre-Processing alternatifs et BM25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tnmyou67808"
      },
      "source": [
        "La fonction qui se charge du pre-processing dans le code de base est text2TokenList. Apportons-y quelques changements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n56VR4j47808"
      },
      "outputs": [],
      "source": [
        "def text2TokenList(text, use_bigrams=True, use_trigrams=True):\n",
        "    stopword = stopwords.words('english') #load the list of english stopwords\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text) # delete characters that are not letters\n",
        "    text = text.lower() #convert words into lowercase\n",
        "    word_tokens = word_tokenize(text) # tokenize our sentences into tokens (words)\n",
        "    word_tokens_filtered = [word for word in word_tokens if word not in stopword and len(word) > 2] #filter stopwords and short words\n",
        "    lemmatizer = WordNetLemmatizer() #lemmatize our tokens\n",
        "    word_tokens_lemmatized = [lemmatizer.lemmatize(token) for token in word_tokens_filtered]\n",
        "    if use_bigrams: #add bigrams\n",
        "        bigrams = list(nltk.bigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(bigram) for bigram in bigrams])\n",
        "    if use_trigrams: #add trigrams\n",
        "        trigrams = list(nltk.trigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(trigram) for trigram in trigrams])\n",
        "    return word_tokens_lemmatized\n",
        "\n",
        "def run_bm25_only(startDoc,endDoc):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\t#150\n",
        "\tndcgTop=10\n",
        "\t#print(\"ndcgTop=\",ndcgTop,\"nbDocsToKeep=\",nbDocsToKeep)\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t#\"\"\"\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k],True,True)\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\t#print(\"doc vocab=\",len(allVocabListDoc))\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicReq[k],True,True)\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tfrom rank_bm25 import BM25Okapi\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k],True,True)\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"reqs...\")\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = text2TokenList(dicReq[k],True,True)\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"bm25 doc indexing...\")\n",
        "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfrom sklearn.metrics import ndcg_score\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\t#get position docId\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\t\t\t\t\t#print(\"TOKEEP=\",docId)\n",
        "\t\t\t\t\t#print(trueDocs)\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\treturn ndcgBM25Cumul\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ici, nous avons amÃ©liorÃ© la fonction de tokenisation en intÃ©grant la lemmatisation, la suppression de caractÃ¨res non alphabÃ©tiques, et l'ajout d'options pour inclure des bigrammes et trigrammes dans le vocabulaire. Ces ajustements permettent de mieux capturer le sens et les relations entre les mots, enrichissant ainsi la reprÃ©sentation des documents et des requÃªtes pour optimiser les performances de BM25."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhhAifVl7808",
        "outputId": "6fe5099a-3e37-404d-a05b-198a5eff0790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RÃ©sultat en prenant seulement 150 documents en compte : 0.7615622891265629\n"
          ]
        }
      ],
      "source": [
        "print(\"RÃ©sultat en prenant seulement 150 documents en compte : \"+str(run_bm25_only(0,150)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBBJx__J7809",
        "outputId": "e4be5a69-afd1-4c27-ccf1-3b55f79d2ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ClÃ© du premier document : MED-118\n",
            "Contenu du premier document : alkylphenols human milk relations dietary habits central taiwan pubmed ncbi abstract aims study determine concentrations num nonylphenol np num octylphenol op num human milk samples examine related factors including mothers demographics dietary habits women consumed median amount cooking oil significantly higher op concentrations num ng/g consumed num ng/g num op concentration significantly consumption cooking oil beta num num fish oil capsules beta num num adjustment age body mass index bmi np concentration significantly consumption fish oil capsules beta num num processed fish products beta num num food pattern cooking oil processed meat products factor analysis strongly op concentration human milk num determinations aid suggesting foods consumption nursing mothers order protect infants np/op exposure num elsevier rights reserved \n",
            "\n",
            "['alkylphenols', 'human', 'milk', 'relation', 'dietary', 'habit', 'central', 'taiwan', 'pubmed', 'ncbi', 'abstract', 'aim', 'study', 'determine', 'concentration', 'num', 'nonylphenol', 'num', 'octylphenol', 'num', 'human', 'milk', 'sample', 'examine', 'related', 'factor', 'including', 'mother', 'demographic', 'dietary', 'habit', 'woman', 'consumed', 'median', 'amount', 'cooking', 'oil', 'significantly', 'higher', 'concentration', 'num', 'consumed', 'num', 'num', 'concentration', 'significantly', 'consumption', 'cooking', 'oil', 'beta', 'num', 'num', 'fish', 'oil', 'capsule', 'beta', 'num', 'num', 'adjustment', 'age', 'body', 'mass', 'index', 'bmi', 'concentration', 'significantly', 'consumption', 'fish', 'oil', 'capsule', 'beta', 'num', 'num', 'processed', 'fish', 'product', 'beta', 'num', 'num', 'food', 'pattern', 'cooking', 'oil', 'processed', 'meat', 'product', 'factor', 'analysis', 'strongly', 'concentration', 'human', 'milk', 'num', 'determination', 'aid', 'suggesting', 'food', 'consumption', 'nursing', 'mother', 'order', 'protect', 'infant', 'exposure', 'num', 'elsevier', 'right', 'reserved', 'alkylphenols human', 'human milk', 'milk relations', 'relations dietary', 'dietary habits', 'habits central', 'central taiwan', 'taiwan pubmed', 'pubmed ncbi', 'ncbi abstract', 'abstract aims', 'aims study', 'study determine', 'determine concentrations', 'concentrations num', 'num nonylphenol', 'nonylphenol num', 'num octylphenol', 'octylphenol num', 'num human', 'human milk', 'milk samples', 'samples examine', 'examine related', 'related factors', 'factors including', 'including mothers', 'mothers demographics', 'demographics dietary', 'dietary habits', 'habits women', 'women consumed', 'consumed median', 'median amount', 'amount cooking', 'cooking oil', 'oil significantly', 'significantly higher', 'higher concentrations', 'concentrations num', 'num consumed', 'consumed num', 'num num', 'num concentration', 'concentration significantly', 'significantly consumption', 'consumption cooking', 'cooking oil', 'oil beta', 'beta num', 'num num', 'num fish', 'fish oil', 'oil capsules', 'capsules beta', 'beta num', 'num num', 'num adjustment', 'adjustment age', 'age body', 'body mass', 'mass index', 'index bmi', 'bmi concentration', 'concentration significantly', 'significantly consumption', 'consumption fish', 'fish oil', 'oil capsules', 'capsules beta', 'beta num', 'num num', 'num processed', 'processed fish', 'fish products', 'products beta', 'beta num', 'num num', 'num food', 'food pattern', 'pattern cooking', 'cooking oil', 'oil processed', 'processed meat', 'meat products', 'products factor', 'factor analysis', 'analysis strongly', 'strongly concentration', 'concentration human', 'human milk', 'milk num', 'num determinations', 'determinations aid', 'aid suggesting', 'suggesting foods', 'foods consumption', 'consumption nursing', 'nursing mothers', 'mothers order', 'order protect', 'protect infants', 'infants exposure', 'exposure num', 'num elsevier', 'elsevier rights', 'rights reserved', 'alkylphenols human milk', 'human milk relations', 'milk relations dietary', 'relations dietary habits', 'dietary habits central', 'habits central taiwan', 'central taiwan pubmed', 'taiwan pubmed ncbi', 'pubmed ncbi abstract', 'ncbi abstract aims', 'abstract aims study', 'aims study determine', 'study determine concentrations', 'determine concentrations num', 'concentrations num nonylphenol', 'num nonylphenol num', 'nonylphenol num octylphenol', 'num octylphenol num', 'octylphenol num human', 'num human milk', 'human milk samples', 'milk samples examine', 'samples examine related', 'examine related factors', 'related factors including', 'factors including mothers', 'including mothers demographics', 'mothers demographics dietary', 'demographics dietary habits', 'dietary habits women', 'habits women consumed', 'women consumed median', 'consumed median amount', 'median amount cooking', 'amount cooking oil', 'cooking oil significantly', 'oil significantly higher', 'significantly higher concentrations', 'higher concentrations num', 'concentrations num consumed', 'num consumed num', 'consumed num num', 'num num concentration', 'num concentration significantly', 'concentration significantly consumption', 'significantly consumption cooking', 'consumption cooking oil', 'cooking oil beta', 'oil beta num', 'beta num num', 'num num fish', 'num fish oil', 'fish oil capsules', 'oil capsules beta', 'capsules beta num', 'beta num num', 'num num adjustment', 'num adjustment age', 'adjustment age body', 'age body mass', 'body mass index', 'mass index bmi', 'index bmi concentration', 'bmi concentration significantly', 'concentration significantly consumption', 'significantly consumption fish', 'consumption fish oil', 'fish oil capsules', 'oil capsules beta', 'capsules beta num', 'beta num num', 'num num processed', 'num processed fish', 'processed fish products', 'fish products beta', 'products beta num', 'beta num num', 'num num food', 'num food pattern', 'food pattern cooking', 'pattern cooking oil', 'cooking oil processed', 'oil processed meat', 'processed meat products', 'meat products factor', 'products factor analysis', 'factor analysis strongly', 'analysis strongly concentration', 'strongly concentration human', 'concentration human milk', 'human milk num', 'milk num determinations', 'num determinations aid', 'determinations aid suggesting', 'aid suggesting foods', 'suggesting foods consumption', 'foods consumption nursing', 'consumption nursing mothers', 'nursing mothers order', 'mothers order protect', 'order protect infants', 'protect infants exposure', 'infants exposure num', 'exposure num elsevier', 'num elsevier rights', 'elsevier rights reserved']\n"
          ]
        }
      ],
      "source": [
        "dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "first_doc_key = next(iter(dicDoc))  #Let's take a look at the first document of our corpus and the output of our pre-processing applicated on this document\n",
        "first_doc_value = dicDoc[first_doc_key]\n",
        "\n",
        "print(f\"ClÃ© du premier document : {first_doc_key}\")\n",
        "print(f\"Contenu du premier document : {first_doc_value}\")\n",
        "print(text2TokenList(first_doc_value, use_bigrams=True, use_trigrams=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPB8ZZ_N7809"
      },
      "source": [
        "### Creation du modÃ¨le word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uQ79u5pn7809"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-msjKs58780-"
      },
      "outputs": [],
      "source": [
        "dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "def text2TokenList(text, use_bigrams=True, use_trigrams=True):\n",
        "    stopword = stopwords.words('english') #load the list of english stopwords\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text) # delete characters that are not letters\n",
        "    text = text.lower() #convert words into lowercase\n",
        "    word_tokens = word_tokenize(text) # tokenize our sentences into tokens (words)\n",
        "    word_tokens_filtered = [word for word in word_tokens if word not in stopword and len(word) > 2] #filter stopwords and short words\n",
        "    lemmatizer = WordNetLemmatizer() #lemmatize our tokens\n",
        "    word_tokens_lemmatized = [lemmatizer.lemmatize(token) for token in word_tokens_filtered]\n",
        "    if use_bigrams: #add bigrams\n",
        "        bigrams = list(nltk.bigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(bigram) for bigram in bigrams])\n",
        "    if use_trigrams: #add trigrams\n",
        "        trigrams = list(nltk.trigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(trigram) for trigram in trigrams])\n",
        "    return word_tokens_lemmatized\n",
        "\n",
        "# tokenize our corpus sentences to words\n",
        "tokenized_corpus = [text2TokenList(doc, use_bigrams=True, use_trigrams=True) for doc in list(dicDoc.values())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le code charge le corpus NFCorpus et dÃ©finit la fonction text2TokenList, qui prÃ©traite et tokenise le texte en supprimant les caractÃ¨res non alphabÃ©tiques, en lemmatizant les mots, et en ajoutant des bigrammes et trigrammes si demandÃ©. Ensuite, il applique cette fonction Ã  tous les documents du corpus pour crÃ©er une liste tokenisÃ©e, enrichissant ainsi la reprÃ©sentation des documents pour une utilisation ultÃ©rieure dans des tÃ¢ches de traitement de texte ou d'analyse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Td4kMZKP780-"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(tokenized_corpus, vector_size=400, window=8, min_count=3, workers=6, sg=0, epochs=100) #create our word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fYcGK-Jg780-"
      },
      "outputs": [],
      "source": [
        "bm25 = BM25Okapi(tokenized_corpus) #apply BM25 to our tokenized corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arG25zIW780-",
        "outputId": "73a4acc1-3aab-44a2-d42a-e9f613df00a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RÃ©sultat du modÃ¨le BM25 avec pre-processing et word2vec : 0.9471492591669831\n"
          ]
        }
      ],
      "source": [
        "def retrieve_documents_bm25(query, bm25, top_n=5):  #Tokenize the query, use BM25 model to compute relevance scores and return the top n documents with the best scores.\n",
        "    query_tokens = text2TokenList(query)\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "    return np.argsort(-bm25_scores)[:top_n]\n",
        "\n",
        "def evaluate_retrieval_bm25(dicReq, dicReqDoc, bm25, doc_ids, top_n=5): #Assesses the retrieval performance of a BM25 model by computing the mean Normalized Discounted Cumulative Gain (NDCG)\n",
        "    ndcg_scores = []\n",
        "    for req_id, query in dicReq.items():\n",
        "        true_scores_dict = dicReqDoc[req_id]\n",
        "        retrieved_doc_indices = retrieve_documents_bm25(query, bm25, top_n=top_n)\n",
        "        retrieved_doc_ids = [doc_ids[idx] for idx in retrieved_doc_indices]\n",
        "        retrieved_scores = np.array([true_scores_dict.get(doc_id, 0) for doc_id in retrieved_doc_ids], dtype=np.float64)\n",
        "        ideal_scores_list = sorted(true_scores_dict.values(), reverse=True)[:len(retrieved_scores)]\n",
        "        ideal_scores_list.extend([0] * (top_n - len(ideal_scores_list)))\n",
        "        ideal_scores = np.array(ideal_scores_list, dtype=np.float64)\n",
        "        ndcg = ndcg_score([ideal_scores], [retrieved_scores], k=top_n)\n",
        "        ndcg_scores.append(ndcg)\n",
        "    return np.mean(ndcg_scores)\n",
        "\n",
        "doc_ids = list(dicDoc.keys())\n",
        "bm25_ndcg = evaluate_retrieval_bm25(dicReq, dicReqDoc, bm25, doc_ids)\n",
        "print(\"RÃ©sultat du modÃ¨le BM25 avec pre-processing et word2vec : \"+str(bm25_ndcg))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
