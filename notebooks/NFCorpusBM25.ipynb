{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-FPcGDygUta"
      },
      "source": [
        "## Loick CUER et Léa FELDMAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTQlDgQY780x"
      },
      "source": [
        "## Natural Language Processing : Information Retrieval Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTqVp4v57804"
      },
      "source": [
        "### Importations des librairies principales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jElt0zoB7805",
        "outputId": "5c408c2d-423f-4f00-8b66-a4072819d5a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     C:\\Users\\loick\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import urllib.request as url_request\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import ndcg_score\n",
        "from rank_bm25 import BM25Okapi\n",
        "nltk.download('all')\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.util import ngrams\n",
        "import random\n",
        "from nltk import ne_chunk, pos_tag\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chargement de NFCorpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KjKr16sB7806"
      },
      "outputs": [],
      "source": [
        "def loadNFCorpus(): #load the NFCorpus\n",
        "    \n",
        "\tdir = \"../data/raw/\"\n",
        "\tfilename = dir +\"dev.docs\"\n",
        "\n",
        "\tdicDoc={}\n",
        "\twith open(filename, 'r', encoding='utf-8') as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\t#print(tabLine)\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\t#print(value)\n",
        "\t\tdicDoc[key] = value\n",
        "\tfilename = dir + \"dev.all.queries\"\n",
        "\tdicReq={}\n",
        "\twith open(filename, 'r', encoding='utf-8') as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\tdicReq[key] = value\n",
        "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
        "\tdicReqDoc=defaultdict(dict)\n",
        "\twith open(filename, 'r', encoding='utf-8') as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.strip().split('\\t')\n",
        "\t\treq = tabLine[0]\n",
        "\t\tdoc = tabLine[2]\n",
        "\t\tscore = int(tabLine[3])\n",
        "\t\tdicReqDoc[req][doc]=score\n",
        "\n",
        "\treturn dicDoc, dicReq, dicReqDoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contenu de dicDoc (5 premiers éléments) :\n",
            "Document ID: MED-118\n",
            "Contenu: alkylphenols human milk relations dietary habits central taiwan pubmed ncbi abstract aims study determine concentrations num nonylphenol np num octylphenol op num human milk samples examine related factors including mothers demographics dietary habits women consumed median amount cooking oil significantly higher op concentrations num ng/g consumed num ng/g num op concentration significantly consumption cooking oil beta num num fish oil capsules beta num num adjustment age body mass index bmi np concentration significantly consumption fish oil capsules beta num num processed fish products beta num num food pattern cooking oil processed meat products factor analysis strongly op concentration human milk num determinations aid suggesting foods consumption nursing mothers order protect infants np/op exposure num elsevier rights reserved \n",
            "...\n",
            "\n",
            "Document ID: MED-329\n",
            "Contenu: phosphate vascular toxin pubmed ncbi abstract elevated phosphate levels advanced renal failure dysregulated calcium parathyroid hormone vitamin levels contribute complex chronic kidney disease-mineral bone disease ckd-mbd converging evidence vitro clinical epidemiological studies suggest increased vascular calcification mortality vessels exposed high conditions vitro develop apoptosis convert bone-like cells develop extensive calcification clinical studies children dialysis show high increased vessel wall thickness arterial stiffness coronary calcification epidemiological studies adult dialysis patients demonstrate significant independent association raised mortality importantly raised cardiovascular pre-dialysis ckd subjects normal renal function high binders effectively reduce serum decrease linked improved survival raised serum triggers release fibroblast growth factor num fgf num beneficial effect increasing excretion early ckd increased num fold dialysis independent cardiovascular risk factor fgf num co-receptor klotho direct effects vasculature leading calcification fascinatingly disturbances fgf num klotho raised premature aging data suggest high levels adverse vascular effects maintaining serum levels normal range reduces cardiovascular risk mortality \n",
            "...\n",
            "\n",
            "Document ID: MED-330\n",
            "Contenu: dietary phosphorus acutely impairs endothelial function abstract excessive dietary phosphorus increase cardiovascular risk healthy individuals patients chronic kidney disease mechanisms underlying risk completely understood determine postprandial hyperphosphatemia promote endothelial dysfunction investigated acute effect phosphorus loading endothelial function vitro vivo exposing bovine aortic endothelial cells phosphorus load increased production reactive oxygen species depended phosphorus influx sodium-dependent phosphate transporters decreased nitric oxide production inhibitory phosphorylation endothelial nitric oxide synthase phosphorus loading inhibited endothelium-dependent vasodilation rat aortic rings num healthy men alternately served meals num mg num mg phosphorus double-blind crossover study measured flow-mediated dilation brachial artery num meals high dietary phosphorus load increased serum phosphorus num significantly decreased flow-mediated dilation flow-mediated dilation correlated inversely serum phosphorus findings suggest endothelial dysfunction mediated acute postprandial hyperphosphatemia contribute relationship serum phosphorus level risk cardiovascular morbidity mortality \n",
            "...\n",
            "\n",
            "Document ID: MED-332\n",
            "Contenu: public health impact dietary phosphorus excess bone cardiovascular health general population pubmed ncbi abstract review explores potential adverse impact increasing phosphorus content american diet renal cardiovascular bone health general population increasingly studies show phosphorus intakes excess nutrient healthy population significantly disrupt hormonal regulation phosphate calcium vitamin contributes disordered mineral metabolism vascular calcification impaired kidney function bone loss large epidemiologic studies suggest mild elevations serum phosphate normal range cardiovascular disease cvd risk healthy populations evidence kidney disease studies linked high dietary phosphorus intake mild serum phosphate nature study design inaccuracies nutrient composition databases phosphorus essential nutrient excess linked tissue damage variety mechanisms involved endocrine regulation extracellular phosphate specifically secretion action fibroblast growth factor num parathyroid hormone disordered regulation hormones high dietary phosphorus key factors contributing renal failure cvd osteoporosis systematically underestimated national surveys phosphorus intake seemingly continues increase result growing consumption highly processed foods restaurant meals fast foods convenience foods increased cumulative ingredients phosphorus food processing merits study shown potential toxicity phosphorus intake exceeds nutrient \n",
            "...\n",
            "\n",
            "Document ID: MED-334\n",
            "Contenu: differences total vitro digestible phosphorus content plant foods beverages pubmed ncbi abstract objective plant foods grain products legumes seeds important sources phosphorus current data content absorbability foods lacking measurement vitro digestible dp content foods reflect absorbability objective study measure total phosphorus tp dp contents selected foods compare amounts tp dp proportion dp tp foods methods tp dp content num foods drinks plant origin measured inductively coupled plasma optical emission spectrometry dp analysis samples digested enzymatically principle alimentary canal analyses popular national brands chosen analysis results highest amount tp num mg num found sesame seeds hull lowest percentage dp num tp cola drinks beer percentage dp tp num num num mg num cereal products highest tp content num mg num dp proportion num present industrial muffins sodium phosphate leavening agent legumes contained average dp content num mg num num tp conclusion absorbability differ substantially plant foods high tp content legumes poor source foods phosphate additives proportion dp high supports previous conclusions effective absorbability additives copyright num national kidney foundation published elsevier rights reserved \n",
            "...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "print(\"Contenu de dicDoc (5 premiers éléments) :\") #affichage des 5 premiers éléments de dicDoc, chaque élément contient l'ID du document et son contenu.\n",
        "for i, (key, value) in enumerate(dicDoc.items()):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"Document ID: {key}\")\n",
        "    print(f\"Contenu: {value[:]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BM25 basique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "45CKUrYP7807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clé du premier document : MED-118\n",
            "Contenu du premier document : alkylphenols human milk relations dietary habits central taiwan pubmed ncbi abstract aims study determine concentrations num nonylphenol np num octylphenol op num human milk samples examine related factors including mothers demographics dietary habits women consumed median amount cooking oil significantly higher op concentrations num ng/g consumed num ng/g num op concentration significantly consumption cooking oil beta num num fish oil capsules beta num num adjustment age body mass index bmi np concentration significantly consumption fish oil capsules beta num num processed fish products beta num num food pattern cooking oil processed meat products factor analysis strongly op concentration human milk num determinations aid suggesting foods consumption nursing mothers order protect infants np/op exposure num elsevier rights reserved \n",
            "\n",
            "['alkylphenols', 'human', 'milk', 'relations', 'dietary', 'habits', 'central', 'taiwan', 'pubmed', 'ncbi', 'abstract', 'aims', 'study', 'determine', 'concentrations', 'num', 'nonylphenol', 'num', 'octylphenol', 'num', 'human', 'milk', 'samples', 'examine', 'related', 'factors', 'including', 'mothers', 'demographics', 'dietary', 'habits', 'women', 'consumed', 'median', 'amount', 'cooking', 'oil', 'significantly', 'higher', 'concentrations', 'num', 'ng/g', 'consumed', 'num', 'ng/g', 'num', 'concentration', 'significantly', 'consumption', 'cooking', 'oil', 'beta', 'num', 'num', 'fish', 'oil', 'capsules', 'beta', 'num', 'num', 'adjustment', 'age', 'body', 'mass', 'index', 'bmi', 'concentration', 'significantly', 'consumption', 'fish', 'oil', 'capsules', 'beta', 'num', 'num', 'processed', 'fish', 'products', 'beta', 'num', 'num', 'food', 'pattern', 'cooking', 'oil', 'processed', 'meat', 'products', 'factor', 'analysis', 'strongly', 'concentration', 'human', 'milk', 'num', 'determinations', 'aid', 'suggesting', 'foods', 'consumption', 'nursing', 'mothers', 'order', 'protect', 'infants', 'np/op', 'exposure', 'num', 'elsevier', 'rights', 'reserved']\n",
            "Résultat en prenant seulement 150 documents en compte : 0.751937759024551\n"
          ]
        }
      ],
      "source": [
        "def text2TokenList(text): #fonction qui prend en entrée un texte et renvoie une liste de mots sans les stopwords\n",
        "\tstopword = set(stopwords.words('english'))\n",
        "\t#print(\"LEN DE STOPWORD=\",len(stopword))\n",
        "\tword_tokens = word_tokenize(text.lower())\n",
        "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
        "\treturn word_tokens_without_stops\n",
        "\n",
        "def run_bm25_only(startDoc,endDoc):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\tndcgTop=10\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t#\"\"\"\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\t#print(\"doc vocab=\",len(allVocabListDoc))\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicReq[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"reqs...\")\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = text2TokenList(dicReq[k])\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"bm25 doc indexing...\")\n",
        "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\t#get position docId\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\t\t\t\t\t#print(\"TOKEEP=\",docId)\n",
        "\t\t\t\t\t#print(trueDocs)\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\treturn ndcgBM25Cumul\n",
        "first_doc_key = next(iter(dicDoc))  #Let's take a look at the first document of our corpus and the output of our pre-processing applicated on this document\n",
        "first_doc_value = dicDoc[first_doc_key]\n",
        "\n",
        "print(f\"Clé du premier document : {first_doc_key}\")\n",
        "print(f\"Contenu du premier document : {first_doc_value}\")\n",
        "print(text2TokenList(first_doc_value))\n",
        "\n",
        "nb_docs = 150\n",
        "print(f\"Résultat en prenant seulement {nb_docs} documents en compte : {run_bm25_only(0, nb_docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG@10 moyen sur l'ensemble du corpus : 0.3735768567999993\n"
          ]
        }
      ],
      "source": [
        "def run_fullbm25_only():\n",
        "    dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "    ndcgTop = 10\n",
        "\n",
        "    corpusDocTokenList = []\n",
        "    corpusDocName = []\n",
        "    corpusDicoDocName = {}\n",
        "\n",
        "    # Prétraitement de tous les documents\n",
        "    for i, (docId, docText) in enumerate(dicDoc.items()):\n",
        "        docTokenList = text2TokenList(docText, True, True)\n",
        "        corpusDocTokenList.append(docTokenList)\n",
        "        corpusDocName.append(docId)\n",
        "        corpusDicoDocName[docId] = i\n",
        "\n",
        "    # Prétraitement de toutes les requêtes\n",
        "    corpusReqTokenList = {}\n",
        "    for reqId, reqText in dicReq.items():\n",
        "        corpusReqTokenList[reqId] = text2TokenList(reqText, True, True)\n",
        "\n",
        "    # Création de l'index BM25\n",
        "    bm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "    ndcgBM25Cumul = 0\n",
        "    nbReq = 0\n",
        "\n",
        "    for reqId, reqTokenList in corpusReqTokenList.items():\n",
        "        doc_scores = bm25.get_scores(reqTokenList)\n",
        "        trueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "        if reqId in dicReqDoc:\n",
        "            for docId, relevance in dicReqDoc[reqId].items():\n",
        "                if docId in corpusDicoDocName:\n",
        "                    posDocId = corpusDicoDocName[docId]\n",
        "                    trueDocs[posDocId] = relevance\n",
        "\n",
        "        # Tri des documents par score décroissant\n",
        "        sorted_indices = np.argsort(doc_scores)[::-1]\n",
        "        sorted_scores = doc_scores[sorted_indices]\n",
        "        sorted_true_docs = trueDocs[sorted_indices]\n",
        "\n",
        "        ndcgBM25Cumul += ndcg_score([sorted_true_docs], [sorted_scores], k=ndcgTop)\n",
        "        nbReq += 1\n",
        "\n",
        "    ndcgBM25Average = ndcgBM25Cumul / nbReq\n",
        "    return ndcgBM25Average\n",
        "\n",
        "# Utilisation de la fonction\n",
        "result = run_fullbm25_only()\n",
        "print(f\"NDCG@10 moyen sur l'ensemble du corpus : {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On constate qu'en utilisant tout le corpus au lieu des 150 premiers documents, le score ndcg est bien plus bas. Cependant cela ne veut pas dire qu le modèle est moins performant. On a icic un score plus réaliste des performances du systeme avec un corpus plus large. + de documents = plus de bruits et plus de difficulté à placer les documents pertinents dans les 10 premières positions. Nous allons continuer d'utiliser bm25 avec 150 documents pour une execution du code plus rapide mais évaluer sur l'ensemble du ccorpus serait plus exact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA8zt0eL7808"
      },
      "source": [
        "### Pre-Processing alternatifs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tnmyou67808"
      },
      "source": [
        "La fonction qui se charge du pre-processing dans le code de base est text2TokenList. Apportons-y quelques changements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n56VR4j47808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clé du premier document : MED-118\n",
            "Contenu du premier document : alkylphenols human milk relations dietary habits central taiwan pubmed ncbi abstract aims study determine concentrations num nonylphenol np num octylphenol op num human milk samples examine related factors including mothers demographics dietary habits women consumed median amount cooking oil significantly higher op concentrations num ng/g consumed num ng/g num op concentration significantly consumption cooking oil beta num num fish oil capsules beta num num adjustment age body mass index bmi np concentration significantly consumption fish oil capsules beta num num processed fish products beta num num food pattern cooking oil processed meat products factor analysis strongly op concentration human milk num determinations aid suggesting foods consumption nursing mothers order protect infants np/op exposure num elsevier rights reserved \n",
            "\n",
            "['alkylphenols', 'human', 'milk', 'relation', 'dietary', 'habit', 'central', 'taiwan', 'pubmed', 'ncbi', 'abstract', 'aim', 'study', 'determine', 'concentration', 'num', 'nonylphenol', 'num', 'octylphenol', 'num', 'human', 'milk', 'sample', 'examine', 'related', 'factor', 'including', 'mother', 'demographic', 'dietary', 'habit', 'woman', 'consumed', 'median', 'amount', 'cooking', 'oil', 'significantly', 'higher', 'concentration', 'num', 'consumed', 'num', 'num', 'concentration', 'significantly', 'consumption', 'cooking', 'oil', 'beta', 'num', 'num', 'fish', 'oil', 'capsule', 'beta', 'num', 'num', 'adjustment', 'age', 'body', 'mass', 'index', 'bmi', 'concentration', 'significantly', 'consumption', 'fish', 'oil', 'capsule', 'beta', 'num', 'num', 'processed', 'fish', 'product', 'beta', 'num', 'num', 'food', 'pattern', 'cooking', 'oil', 'processed', 'meat', 'product', 'factor', 'analysis', 'strongly', 'concentration', 'human', 'milk', 'num', 'determination', 'aid', 'suggesting', 'food', 'consumption', 'nursing', 'mother', 'order', 'protect', 'infant', 'exposure', 'num', 'elsevier', 'right', 'reserved', 'alkylphenols human', 'human milk', 'milk relations', 'relations dietary', 'dietary habits', 'habits central', 'central taiwan', 'taiwan pubmed', 'pubmed ncbi', 'ncbi abstract', 'abstract aims', 'aims study', 'study determine', 'determine concentrations', 'concentrations num', 'num nonylphenol', 'nonylphenol num', 'num octylphenol', 'octylphenol num', 'num human', 'human milk', 'milk samples', 'samples examine', 'examine related', 'related factors', 'factors including', 'including mothers', 'mothers demographics', 'demographics dietary', 'dietary habits', 'habits women', 'women consumed', 'consumed median', 'median amount', 'amount cooking', 'cooking oil', 'oil significantly', 'significantly higher', 'higher concentrations', 'concentrations num', 'num consumed', 'consumed num', 'num num', 'num concentration', 'concentration significantly', 'significantly consumption', 'consumption cooking', 'cooking oil', 'oil beta', 'beta num', 'num num', 'num fish', 'fish oil', 'oil capsules', 'capsules beta', 'beta num', 'num num', 'num adjustment', 'adjustment age', 'age body', 'body mass', 'mass index', 'index bmi', 'bmi concentration', 'concentration significantly', 'significantly consumption', 'consumption fish', 'fish oil', 'oil capsules', 'capsules beta', 'beta num', 'num num', 'num processed', 'processed fish', 'fish products', 'products beta', 'beta num', 'num num', 'num food', 'food pattern', 'pattern cooking', 'cooking oil', 'oil processed', 'processed meat', 'meat products', 'products factor', 'factor analysis', 'analysis strongly', 'strongly concentration', 'concentration human', 'human milk', 'milk num', 'num determinations', 'determinations aid', 'aid suggesting', 'suggesting foods', 'foods consumption', 'consumption nursing', 'nursing mothers', 'mothers order', 'order protect', 'protect infants', 'infants exposure', 'exposure num', 'num elsevier', 'elsevier rights', 'rights reserved', 'alkylphenols human milk', 'human milk relations', 'milk relations dietary', 'relations dietary habits', 'dietary habits central', 'habits central taiwan', 'central taiwan pubmed', 'taiwan pubmed ncbi', 'pubmed ncbi abstract', 'ncbi abstract aims', 'abstract aims study', 'aims study determine', 'study determine concentrations', 'determine concentrations num', 'concentrations num nonylphenol', 'num nonylphenol num', 'nonylphenol num octylphenol', 'num octylphenol num', 'octylphenol num human', 'num human milk', 'human milk samples', 'milk samples examine', 'samples examine related', 'examine related factors', 'related factors including', 'factors including mothers', 'including mothers demographics', 'mothers demographics dietary', 'demographics dietary habits', 'dietary habits women', 'habits women consumed', 'women consumed median', 'consumed median amount', 'median amount cooking', 'amount cooking oil', 'cooking oil significantly', 'oil significantly higher', 'significantly higher concentrations', 'higher concentrations num', 'concentrations num consumed', 'num consumed num', 'consumed num num', 'num num concentration', 'num concentration significantly', 'concentration significantly consumption', 'significantly consumption cooking', 'consumption cooking oil', 'cooking oil beta', 'oil beta num', 'beta num num', 'num num fish', 'num fish oil', 'fish oil capsules', 'oil capsules beta', 'capsules beta num', 'beta num num', 'num num adjustment', 'num adjustment age', 'adjustment age body', 'age body mass', 'body mass index', 'mass index bmi', 'index bmi concentration', 'bmi concentration significantly', 'concentration significantly consumption', 'significantly consumption fish', 'consumption fish oil', 'fish oil capsules', 'oil capsules beta', 'capsules beta num', 'beta num num', 'num num processed', 'num processed fish', 'processed fish products', 'fish products beta', 'products beta num', 'beta num num', 'num num food', 'num food pattern', 'food pattern cooking', 'pattern cooking oil', 'cooking oil processed', 'oil processed meat', 'processed meat products', 'meat products factor', 'products factor analysis', 'factor analysis strongly', 'analysis strongly concentration', 'strongly concentration human', 'concentration human milk', 'human milk num', 'milk num determinations', 'num determinations aid', 'determinations aid suggesting', 'aid suggesting foods', 'suggesting foods consumption', 'foods consumption nursing', 'consumption nursing mothers', 'nursing mothers order', 'mothers order protect', 'order protect infants', 'protect infants exposure', 'infants exposure num', 'exposure num elsevier', 'num elsevier rights', 'elsevier rights reserved']\n",
            "Résultat en prenant seulement 150 documents en compte : 0.7615622891265629\n"
          ]
        }
      ],
      "source": [
        "def text2TokenList2(text, use_bigrams=True, use_trigrams=True):\n",
        "    stopword = set(stopwords.words('english')) #load the list of english stopwords\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text) # delete characters that are not letters\n",
        "    text = text.lower() #convert words into lowercase\n",
        "    word_tokens = word_tokenize(text) # tokenize our sentences into tokens (words)\n",
        "    word_tokens_filtered = [word for word in word_tokens if word not in stopword and len(word) > 2] #filter stopwords and short words\n",
        "    lemmatizer = WordNetLemmatizer() #lemmatize our tokens\n",
        "    word_tokens_lemmatized = [lemmatizer.lemmatize(token) for token in word_tokens_filtered]\n",
        "    if use_bigrams: #add bigrams\n",
        "        bigrams = list(nltk.bigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(bigram) for bigram in bigrams])\n",
        "    if use_trigrams: #add trigrams\n",
        "        trigrams = list(nltk.trigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend([' '.join(trigram) for trigram in trigrams])\n",
        "    return word_tokens_lemmatized\n",
        "\n",
        "def run_bm25_only2(startDoc,endDoc):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\tndcgTop=10\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t#\"\"\"\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList2(dicDoc[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\t#print(\"doc vocab=\",len(allVocabListDoc))\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = text2TokenList2(dicReq[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList2(dicDoc[k])\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"reqs...\")\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = text2TokenList2(dicReq[k])\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"bm25 doc indexing...\")\n",
        "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\t#get position docId\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\t\t\t\t\t#print(\"TOKEEP=\",docId)\n",
        "\t\t\t\t\t#print(trueDocs)\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\treturn ndcgBM25Cumul\n",
        "\n",
        "first_doc_key = next(iter(dicDoc))  #Let's take a look at the first document of our corpus and the output of our pre-processing applicated on this document\n",
        "first_doc_value = dicDoc[first_doc_key]\n",
        "\n",
        "print(f\"Clé du premier document : {first_doc_key}\")\n",
        "print(f\"Contenu du premier document : {first_doc_value}\")\n",
        "print(text2TokenList2(first_doc_value, use_bigrams=True, use_trigrams=True))\n",
        "\n",
        "\n",
        "nb_docs = 150\n",
        "print(f\"Résultat en prenant seulement {nb_docs} documents en compte : {run_bm25_only2(0, nb_docs)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ici, nous avons amélioré la fonction de tokenisation en intégrant la lemmatisation, la suppression de caractères non alphabétiques, et l'ajout d'options pour inclure des bigrammes et trigrammes dans le vocabulaire. Ces ajustements permettent de mieux capturer le sens et les relations entre les mots, enrichissant ainsi la représentation des documents et des requêtes pour optimiser les performances de BM25."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On constate que de  nombreux mots parasites sont tokenisés comme \"num\" ou \"op\", on va donc les filtrer en choississant de ne conservere que les mots de 4 lettres et +"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clé du premier document : MED-118\n",
            "Contenu du premier document : alkylphenols human milk relations dietary habits central taiwan pubmed ncbi abstract aims study determine concentrations num nonylphenol np num octylphenol op num human milk samples examine related factors including mothers demographics dietary habits women consumed median amount cooking oil significantly higher op concentrations num ng/g consumed num ng/g num op concentration significantly consumption cooking oil beta num num fish oil capsules beta num num adjustment age body mass index bmi np concentration significantly consumption fish oil capsules beta num num processed fish products beta num num food pattern cooking oil processed meat products factor analysis strongly op concentration human milk num determinations aid suggesting foods consumption nursing mothers order protect infants np/op exposure num elsevier rights reserved \n",
            "\n",
            "['alkylphenols', 'human', 'milk', 'relation', 'dietary', 'habit', 'central', 'taiwan', 'pubmed', 'ncbi', 'abstract', 'aim', 'study', 'determine', 'concentration', 'nonylphenol', 'octylphenol', 'human', 'milk', 'sample', 'examine', 'related', 'factor', 'including', 'mother', 'demographic', 'dietary', 'habit', 'woman', 'consumed', 'median', 'amount', 'cooking', 'significantly', 'higher', 'concentration', 'consumed', 'concentration', 'significantly', 'consumption', 'cooking', 'beta', 'fish', 'capsule', 'beta', 'adjustment', 'body', 'mass', 'index', 'concentration', 'significantly', 'consumption', 'fish', 'capsule', 'beta', 'processed', 'fish', 'product', 'beta', 'food', 'pattern', 'cooking', 'processed', 'meat', 'product', 'factor', 'analysis', 'strongly', 'concentration', 'human', 'milk', 'determination', 'suggesting', 'food', 'consumption', 'nursing', 'mother', 'order', 'protect', 'infant', 'exposure', 'elsevier', 'right', 'reserved', 'alkylphenols_human', 'human_milk', 'milk_relations', 'relations_dietary', 'dietary_habits', 'habits_central', 'central_taiwan', 'taiwan_pubmed', 'pubmed_ncbi', 'ncbi_abstract', 'abstract_aims', 'aims_study', 'study_determine', 'determine_concentrations', 'concentrations_nonylphenol', 'nonylphenol_octylphenol', 'octylphenol_human', 'human_milk', 'milk_samples', 'samples_examine', 'examine_related', 'related_factors', 'factors_including', 'including_mothers', 'mothers_demographics', 'demographics_dietary', 'dietary_habits', 'habits_women', 'women_consumed', 'consumed_median', 'median_amount', 'amount_cooking', 'cooking_significantly', 'significantly_higher', 'higher_concentrations', 'concentrations_consumed', 'consumed_concentration', 'concentration_significantly', 'significantly_consumption', 'consumption_cooking', 'cooking_beta', 'beta_fish', 'fish_capsules', 'capsules_beta', 'beta_adjustment', 'adjustment_body', 'body_mass', 'mass_index', 'index_concentration', 'concentration_significantly', 'significantly_consumption', 'consumption_fish', 'fish_capsules', 'capsules_beta', 'beta_processed', 'processed_fish', 'fish_products', 'products_beta', 'beta_food', 'food_pattern', 'pattern_cooking', 'cooking_processed', 'processed_meat', 'meat_products', 'products_factor', 'factor_analysis', 'analysis_strongly', 'strongly_concentration', 'concentration_human', 'human_milk', 'milk_determinations', 'determinations_suggesting', 'suggesting_foods', 'foods_consumption', 'consumption_nursing', 'nursing_mothers', 'mothers_order', 'order_protect', 'protect_infants', 'infants_exposure', 'exposure_elsevier', 'elsevier_rights', 'rights_reserved', 'alkylphenols_human_milk', 'human_milk_relations', 'milk_relations_dietary', 'relations_dietary_habits', 'dietary_habits_central', 'habits_central_taiwan', 'central_taiwan_pubmed', 'taiwan_pubmed_ncbi', 'pubmed_ncbi_abstract', 'ncbi_abstract_aims', 'abstract_aims_study', 'aims_study_determine', 'study_determine_concentrations', 'determine_concentrations_nonylphenol', 'concentrations_nonylphenol_octylphenol', 'nonylphenol_octylphenol_human', 'octylphenol_human_milk', 'human_milk_samples', 'milk_samples_examine', 'samples_examine_related', 'examine_related_factors', 'related_factors_including', 'factors_including_mothers', 'including_mothers_demographics', 'mothers_demographics_dietary', 'demographics_dietary_habits', 'dietary_habits_women', 'habits_women_consumed', 'women_consumed_median', 'consumed_median_amount', 'median_amount_cooking', 'amount_cooking_significantly', 'cooking_significantly_higher', 'significantly_higher_concentrations', 'higher_concentrations_consumed', 'concentrations_consumed_concentration', 'consumed_concentration_significantly', 'concentration_significantly_consumption', 'significantly_consumption_cooking', 'consumption_cooking_beta', 'cooking_beta_fish', 'beta_fish_capsules', 'fish_capsules_beta', 'capsules_beta_adjustment', 'beta_adjustment_body', 'adjustment_body_mass', 'body_mass_index', 'mass_index_concentration', 'index_concentration_significantly', 'concentration_significantly_consumption', 'significantly_consumption_fish', 'consumption_fish_capsules', 'fish_capsules_beta', 'capsules_beta_processed', 'beta_processed_fish', 'processed_fish_products', 'fish_products_beta', 'products_beta_food', 'beta_food_pattern', 'food_pattern_cooking', 'pattern_cooking_processed', 'cooking_processed_meat', 'processed_meat_products', 'meat_products_factor', 'products_factor_analysis', 'factor_analysis_strongly', 'analysis_strongly_concentration', 'strongly_concentration_human', 'concentration_human_milk', 'human_milk_determinations', 'milk_determinations_suggesting', 'determinations_suggesting_foods', 'suggesting_foods_consumption', 'foods_consumption_nursing', 'consumption_nursing_mothers', 'nursing_mothers_order', 'mothers_order_protect', 'order_protect_infants', 'protect_infants_exposure', 'infants_exposure_elsevier', 'exposure_elsevier_rights', 'elsevier_rights_reserved']\n",
            "Résultat en prenant seulement 150 documents en compte : 0.7642905870045049\n"
          ]
        }
      ],
      "source": [
        "def text2TokenList3(text, use_bigrams=True, use_trigrams=True):\n",
        "    stopword = set(stopwords.words('english')) #load the list of english stopwords\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text) # delete characters that are not letters\n",
        "    text = text.lower() #convert words into lowercase\n",
        "    word_tokens = word_tokenize(text) # tokenize our sentences into tokens (words)\n",
        "    word_tokens_filtered = [word for word in word_tokens if word not in stopword and len(word) > 3] #filter stopwords and words of 3 letters or less\n",
        "    lemmatizer = WordNetLemmatizer() #lemmatize our tokens\n",
        "    word_tokens_lemmatized = [lemmatizer.lemmatize(token) for token in word_tokens_filtered]\n",
        "    if use_bigrams: #add bigrams\n",
        "        bigrams = list(nltk.bigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend(['_'.join(bigram) for bigram in bigrams])\n",
        "    if use_trigrams: #add trigrams\n",
        "        trigrams = list(nltk.trigrams(word_tokens_filtered))\n",
        "        word_tokens_lemmatized.extend(['_'.join(trigram) for trigram in trigrams])\n",
        "    return word_tokens_lemmatized\n",
        "\n",
        "def run_bm25_only3(startDoc,endDoc):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\tndcgTop=10\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t#\"\"\"\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList3(dicDoc[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\t#print(\"doc vocab=\",len(allVocabListDoc))\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = text2TokenList3(dicReq[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList3(dicDoc[k])\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"reqs...\")\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = text2TokenList3(dicReq[k])\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"bm25 doc indexing...\")\n",
        "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\t#get position docId\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\t\t\t\t\t#print(\"TOKEEP=\",docId)\n",
        "\t\t\t\t\t#print(trueDocs)\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\treturn ndcgBM25Cumul\n",
        "\n",
        "first_doc_key = next(iter(dicDoc))  #Let's take a look at the first document of our corpus and the output of our pre-processing applicated on this document\n",
        "first_doc_value = dicDoc[first_doc_key]\n",
        "\n",
        "print(f\"Clé du premier document : {first_doc_key}\")\n",
        "print(f\"Contenu du premier document : {first_doc_value}\")\n",
        "print(text2TokenList3(first_doc_value, use_bigrams=True, use_trigrams=True))\n",
        "\n",
        "\n",
        "nb_docs = 150\n",
        "print(f\"Résultat en prenant seulement {nb_docs} documents en compte : {run_bm25_only3(0, nb_docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clé du premier document : MED-118\n",
            "Contenu du premier document : alkylphenols human milk relations dietary habits central taiwan pubmed ncbi abstract aims study determine concentrations num nonylphenol np num octylphenol op num human milk samples examine related factors including mothers demographics dietary habits women consumed median amount cooking oil significantly higher op concentrations num ng/g consumed num ng/g num op concentration significantly consumption cooking oil beta num num fish oil capsules beta num num adjustment age body mass index bmi np concentration significantly consumption fish oil capsules beta num num processed fish products beta num num food pattern cooking oil processed meat products factor analysis strongly op concentration human milk num determinations aid suggesting foods consumption nursing mothers order protect infants np/op exposure num elsevier rights reserved \n",
            "\n",
            "['alkylphenols', 'human', 'milk', 'relation', 'dietary', 'habit', 'central', 'taiwan', 'pubmed', 'ncbi', 'abstract', 'aim', 'study', 'determine', 'concentration', 'num', 'nonylphenol', 'num', 'octylphenol', 'num', 'human', 'milk', 'sample', 'examine', 'related', 'factor', 'including', 'mother', 'demographic', 'dietary', 'habit', 'woman', 'consumed', 'median', 'amount', 'cooking', 'oil', 'significantly', 'higher', 'concentration', 'num', 'consumed', 'num', 'num', 'concentration', 'significantly', 'consumption', 'cooking', 'oil', 'beta', 'num', 'num', 'fish', 'oil', 'capsule', 'beta', 'num', 'num', 'adjustment', 'age', 'body', 'mass', 'index', 'bmi', 'concentration', 'significantly', 'consumption', 'fish', 'oil', 'capsule', 'beta', 'num', 'num', 'processed', 'fish', 'product', 'beta', 'num', 'num', 'food', 'pattern', 'cooking', 'oil', 'processed', 'meat', 'product', 'factor', 'analysis', 'strongly', 'concentration', 'human', 'milk', 'num', 'determination', 'aid', 'suggesting', 'food', 'consumption', 'nursing', 'mother', 'order', 'protect', 'infant', 'exposure', 'num', 'elsevier', 'right', 'reserved', 'human_milk', 'dietary_habits', 'concentrations_num', 'cooking_oil', 'num_num', 'concentration_significantly', 'significantly_consumption', 'beta_num', 'fish_oil', 'oil_capsules', 'capsules_beta', 'concentration_significantly_consumption', 'beta_num_num', 'fish_oil_capsules', 'oil_capsules_beta', 'capsules_beta_num']\n",
            "Résultat en prenant seulement 150 documents en compte : 0.7652464419314504\n"
          ]
        }
      ],
      "source": [
        "def text2TokenList4(text, use_bigrams=True, use_trigrams=True): #add inteligent n-grams\n",
        "    # Charger la liste des stopwords anglais\n",
        "    stopword = set(stopwords.words('english'))\n",
        "    \n",
        "    # Prétraitement du texte\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Conserver les chiffres, remplacer la ponctuation par des espaces\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Tokenisation\n",
        "    word_tokens = word_tokenize(text)\n",
        "    \n",
        "    # Filtrage des stopwords et des mots courts\n",
        "    word_tokens_filtered = [word for word in word_tokens if word not in stopword and len(word) > 2]\n",
        "    \n",
        "    # Lemmatisation\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_tokens_lemmatized = [lemmatizer.lemmatize(token) for token in word_tokens_filtered]\n",
        "    \n",
        "    # Ajout des n-grammes pertinents\n",
        "    if use_bigrams or use_trigrams:\n",
        "        n_grams = []\n",
        "        if use_bigrams:\n",
        "            n_grams.extend(ngrams(word_tokens_filtered, 2))\n",
        "        if use_trigrams:\n",
        "            n_grams.extend(ngrams(word_tokens_filtered, 3))\n",
        "        \n",
        "        # Filtrer les n-grammes pertinents (par exemple, ceux qui apparaissent plus d'une fois)\n",
        "        n_gram_freq = nltk.FreqDist(n_grams)\n",
        "        relevant_n_grams = [ng for ng, freq in n_gram_freq.items() if freq > 1]\n",
        "        \n",
        "        # Ajouter les n-grammes pertinents à la liste des tokens\n",
        "        word_tokens_lemmatized.extend(['_'.join(ng) for ng in relevant_n_grams])\n",
        "    \n",
        "    return word_tokens_lemmatized\n",
        "\n",
        "def run_bm25_only4(startDoc,endDoc):\n",
        "\n",
        "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "\tdocsToKeep=[]\n",
        "\treqsToKeep=[]\n",
        "\tdicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "\tndcgTop=10\n",
        "\n",
        "\ti=startDoc\n",
        "\tfor reqId in dicReqDoc:\n",
        "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "\t\t\tbreak\n",
        "\t\tfor docId in dicReqDoc[reqId]:\n",
        "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "\t\t\tdocsToKeep.append(docId)\n",
        "\t\t\ti = i + 1\n",
        "\t\treqsToKeep.append(reqId)\n",
        "\tdocsToKeep = list(set(docsToKeep))\n",
        "\n",
        "\t#\"\"\"\n",
        "\tallVocab ={}\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList4(dicDoc[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListDoc = list(allVocab)\n",
        "\t#print(\"doc vocab=\",len(allVocabListDoc))\n",
        "\tallVocab ={}\n",
        "\tfor k in reqsToKeep:\n",
        "\t\tdocTokenList = text2TokenList4(dicReq[k])\n",
        "\t\t#print(docTokenList)\n",
        "\t\tfor word in docTokenList:\n",
        "\t\t\tif word not in allVocab:\n",
        "\t\t\t\tallVocab[word] = word\n",
        "\tallVocabListReq = list(allVocab)\n",
        "\n",
        "\tcorpusDocTokenList = []\n",
        "\tcorpusReqTokenList = {}\n",
        "\tcorpusDocName=[]\n",
        "\tcorpusDicoDocName={}\n",
        "\ti = 0\n",
        "\tfor k in docsToKeep:\n",
        "\t\tdocTokenList = text2TokenList4(dicDoc[k])\n",
        "\t\tcorpusDocTokenList.append(docTokenList)\n",
        "\t\tcorpusDocName.append(k)\n",
        "\t\tcorpusDicoDocName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"reqs...\")\n",
        "\tcorpusReqName=[]\n",
        "\tcorpusDicoReqName={}\n",
        "\ti = 0\n",
        "\tfor k in reqsToKeep:\n",
        "\t\treqTokenList = text2TokenList4(dicReq[k])\n",
        "\t\tcorpusReqTokenList[k] = reqTokenList\n",
        "\t\tcorpusReqName.append(k)\n",
        "\t\tcorpusDicoReqName[k] = i\n",
        "\t\ti = i + 1\n",
        "\n",
        "\t#print(\"bm25 doc indexing...\")\n",
        "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
        "\n",
        "\tndcgCumul=0\n",
        "\tcorpusReqVec={}\n",
        "\tndcgBM25Cumul=0\n",
        "\tnbReq=0\n",
        "\n",
        "\tfor req in corpusReqTokenList:\n",
        "\t\tj=0\n",
        "\t\treqTokenList = corpusReqTokenList[req]\n",
        "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
        "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "\t\tfor docId in corpusDicoDocName:\n",
        "\t\t\tif req in dicReqDocToKeep:\n",
        "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
        "\t\t\t\t\t#get position docId\n",
        "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
        "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
        "\t\t\t\t\t#print(\"TOKEEP=\",docId)\n",
        "\t\t\t\t\t#print(trueDocs)\n",
        "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "\t\tnbReq = nbReq + 1\n",
        "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "\treturn ndcgBM25Cumul\n",
        "\n",
        "first_doc_key = next(iter(dicDoc))  #Let's take a look at the first document of our corpus and the output of our pre-processing applicated on this document\n",
        "first_doc_value = dicDoc[first_doc_key]\n",
        "\n",
        "print(f\"Clé du premier document : {first_doc_key}\")\n",
        "print(f\"Contenu du premier document : {first_doc_value}\")\n",
        "print(text2TokenList4(first_doc_value, use_bigrams=True, use_trigrams=True))\n",
        "\n",
        "\n",
        "nb_docs = 150\n",
        "print(f\"Résultat en prenant seulement {nb_docs} documents en compte : {run_bm25_only4(0, nb_docs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "gridsearch for bm25 hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meilleurs paramètres : k1=1.5, b=0.9\n",
            "Score final avec les meilleurs paramètres : 0.7848648682406477\n"
          ]
        }
      ],
      "source": [
        "def run_bm25_with_params(startDoc, endDoc, k1, b):\n",
        "    dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "\n",
        "    docsToKeep = []\n",
        "    reqsToKeep = []\n",
        "    dicReqDocToKeep = defaultdict(dict)\n",
        "\n",
        "    ndcgTop = 10\n",
        "\n",
        "    i = startDoc\n",
        "    for reqId in dicReqDoc:\n",
        "        if i > (endDoc - startDoc):\n",
        "            break\n",
        "        for docId in dicReqDoc[reqId]:\n",
        "            dicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "            docsToKeep.append(docId)\n",
        "            i += 1\n",
        "        reqsToKeep.append(reqId)\n",
        "    docsToKeep = list(set(docsToKeep))\n",
        "\n",
        "    corpusDocTokenList = []\n",
        "    corpusReqTokenList = {}\n",
        "    corpusDocName = []\n",
        "    corpusDicoDocName = {}\n",
        "\n",
        "    for i, k in enumerate(docsToKeep):\n",
        "        docTokenList = text2TokenList4(dicDoc[k])\n",
        "        corpusDocTokenList.append(docTokenList)\n",
        "        corpusDocName.append(k)\n",
        "        corpusDicoDocName[k] = i\n",
        "\n",
        "    for k in reqsToKeep:\n",
        "        reqTokenList = text2TokenList4(dicReq[k])\n",
        "        corpusReqTokenList[k] = reqTokenList\n",
        "\n",
        "    bm25 = BM25Okapi(corpusDocTokenList, k1=k1, b=b)\n",
        "\n",
        "    ndcgBM25Cumul = 0\n",
        "    nbReq = 0\n",
        "\n",
        "    for req in corpusReqTokenList:\n",
        "        reqTokenList = corpusReqTokenList[req]\n",
        "        doc_scores = bm25.get_scores(reqTokenList)\n",
        "        trueDocs = np.zeros(len(corpusDocTokenList))\n",
        "\n",
        "        if req in dicReqDocToKeep:\n",
        "            for docId, relevance in dicReqDocToKeep[req].items():\n",
        "                if docId in corpusDicoDocName:\n",
        "                    posDocId = corpusDicoDocName[docId]\n",
        "                    trueDocs[posDocId] = relevance\n",
        "\n",
        "        ndcgBM25Cumul += ndcg_score([trueDocs], [doc_scores], k=ndcgTop)\n",
        "        nbReq += 1\n",
        "\n",
        "    ndcgBM25Cumul /= nbReq\n",
        "    return ndcgBM25Cumul\n",
        "\n",
        "# Définir les valeurs à tester pour k1 et b\n",
        "k1_values = [0.5, 1.0, 1.2, 1.5, 2.0]\n",
        "b_values = [0.5, 0.6, 0.75, 0.8, 0.9]\n",
        "\n",
        "best_score = 0\n",
        "best_params = None\n",
        "\n",
        "nb_docs = 150  # Nombre de documents à considérer\n",
        "\n",
        "for k1 in k1_values:\n",
        "    for b in b_values:\n",
        "        score = run_bm25_with_params(0, nb_docs, k1, b)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = (k1, b)\n",
        "\n",
        "print(f\"Meilleurs paramètres : k1={best_params[0]}, b={best_params[1]}\")\n",
        "\n",
        "# Exécuter avec les meilleurs paramètres\n",
        "final_score = run_bm25_with_params(0, nb_docs, best_params[0], best_params[1])\n",
        "print(f\"Score final avec les meilleurs paramètres : {final_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPB8ZZ_N7809"
      },
      "source": [
        "### Creation du modèle word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uQ79u5pn7809"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-msjKs58780-"
      },
      "outputs": [],
      "source": [
        "tokenized_corpus = [text2TokenList(doc, use_bigrams=True, use_trigrams=True) for doc in list(dicDoc.values())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le code charge le corpus NFCorpus et définit la fonction text2TokenList, qui prétraite et tokenise le texte en supprimant les caractères non alphabétiques, en lemmatizant les mots, et en ajoutant des bigrammes et trigrammes si demandé. Ensuite, il applique cette fonction à tous les documents du corpus pour créer une liste tokenisée, enrichissant ainsi la représentation des documents pour une utilisation ultérieure dans des tâches de traitement de texte ou d'analyse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Td4kMZKP780-"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(tokenized_corpus, vector_size=400, window=8, min_count=3, workers=6, sg=0, epochs=100) #create our word2vec model\n",
        "bm25 = BM25Okapi(tokenized_corpus) #apply BM25 to our tokenized corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arG25zIW780-",
        "outputId": "73a4acc1-3aab-44a2-d42a-e9f613df00a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultat du modèle BM25 avec pre-processing et word2vec : 0.9480280287435777\n"
          ]
        }
      ],
      "source": [
        "def retrieve_documents_bm25_w2v(query, bm25, model, tokenized_corpus, top_n=5):\n",
        "    query_tokens = text2TokenList(query)\n",
        "    bm25_scores = bm25.get_scores(query_tokens)\n",
        "    \n",
        "    # Calculer la similarité Word2Vec\n",
        "    query_vec = np.mean([model.wv[word] for word in query_tokens if word in model.wv], axis=0)\n",
        "    w2v_scores = np.array([np.dot(query_vec, np.mean([model.wv[word] for word in doc if word in model.wv], axis=0)) for doc in tokenized_corpus])\n",
        "    \n",
        "    # Normaliser les scores\n",
        "    bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))\n",
        "    w2v_scores = (w2v_scores - np.min(w2v_scores)) / (np.max(w2v_scores) - np.min(w2v_scores))\n",
        "    \n",
        "    # Combiner les scores (vous pouvez ajuster les poids)\n",
        "    combined_scores = 0.7 * bm25_scores + 0.3 * w2v_scores\n",
        "    return np.argsort(-combined_scores)[:top_n]\n",
        "\n",
        "def evaluate_retrieval_bm25(dicReq, dicReqDoc, bm25, doc_ids, model, tokenized_corpus, top_n=5):\n",
        "    ndcg_scores = []\n",
        "    for req_id, query in dicReq.items():\n",
        "        true_scores_dict = dicReqDoc[req_id]\n",
        "        retrieved_doc_indices = retrieve_documents_bm25_w2v(query, bm25, model, tokenized_corpus, top_n=top_n)\n",
        "        retrieved_doc_ids = [doc_ids[idx] for idx in retrieved_doc_indices]\n",
        "        retrieved_scores = np.array([true_scores_dict.get(doc_id, 0) for doc_id in retrieved_doc_ids], dtype=np.float64)\n",
        "        ideal_scores_list = sorted(true_scores_dict.values(), reverse=True)[:len(retrieved_scores)]\n",
        "        ideal_scores_list.extend([0] * (top_n - len(ideal_scores_list)))\n",
        "        ideal_scores = np.array(ideal_scores_list, dtype=np.float64)\n",
        "        ndcg = ndcg_score([ideal_scores], [retrieved_scores], k=top_n)\n",
        "        ndcg_scores.append(ndcg)\n",
        "    return np.mean(ndcg_scores)\n",
        "\n",
        "doc_ids = list(dicDoc.keys())\n",
        "bm25_ndcg = evaluate_retrieval_bm25(dicReq, dicReqDoc, bm25, doc_ids, model, tokenized_corpus)\n",
        "print(\"Résultat du modèle BM25 avec pre-processing et word2vec : \"+str(bm25_ndcg))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
